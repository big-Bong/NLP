{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/vc/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = pd.read_csv('abcnews-date-text.csv')\n",
    "input_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = input_file[['headline_text']]\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_token(token):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    token_arr = []\n",
    "    for tokens in simple_preprocess(text,min_len=3):\n",
    "        if tokens not in STOPWORDS:\n",
    "            token_arr.append(lemmatize_token(tokens))\n",
    "    \n",
    "    return token_arr\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aba decides against community broadcasting licence\n",
      "['aba', 'decides', 'community', 'broadcasting', 'licence']\n",
      "act fire witnesses must be aware of defamation\n",
      "['act', 'witness', 'aware', 'defamation']\n",
      "a g calls for infrastructure protection summit\n",
      "['call', 'infrastructure', 'protection', 'summit']\n",
      "air nz staff in aust strike for pay rise\n",
      "['air', 'staff', 'aust', 'strike', 'pay', 'rise']\n",
      "air nz strike to affect australian travellers\n",
      "['air', 'strike', 'affect', 'australian', 'traveller']\n"
     ]
    }
   ],
   "source": [
    "sample_doc = documents[:5]\n",
    "for val in sample_doc['headline_text']:\n",
    "    print(val)\n",
    "    token_arr = preprocessing(val)\n",
    "    print(token_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [aba, decides, community, broadcasting, licence]\n",
       "1                   [act, witness, aware, defamation]\n",
       "2          [call, infrastructure, protection, summit]\n",
       "3               [air, staff, aust, strike, pay, rise]\n",
       "4        [air, strike, affect, australian, traveller]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['headline_text'].map(preprocessing)\n",
    "processed_docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for k,v in dictionary.iteritems():\n",
    "#    print(k)\n",
    "#    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below = 15, no_above = 0.5, keep_n = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['irish', 'man', 'arrested', 'omagh', 'bombing', 'bombing', 'man']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[78]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['israeli', 'force', 'push', 'gaza', 'strip']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(295, 2), (306, 1), (307, 2), (308, 1), (309, 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus[78]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(314, 1), (315, 1), (316, 1), (317, 1), (318, 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus[80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.5296211440960563),\n",
       " (1, 0.5296211440960563),\n",
       " (2, 0.2805238101225404),\n",
       " (3, 0.47735329175810276),\n",
       " (4, 0.36392734749508066)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_bow = models.LdaMulticore(bow_corpus,num_topics=10,id2word=dictionary,passes=2,workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.033*\"government\" + 0.021*\"coast\" + 0.013*\"price\" + 0.013*\"gold\" + 0.013*\"say\" + 0.013*\"league\" + 0.012*\"rise\" + 0.012*\"live\" + 0.011*\"rate\" + 0.010*\"nrl\"\n",
      "Topic: 1 \n",
      "Words: 0.017*\"market\" + 0.015*\"australian\" + 0.012*\"state\" + 0.011*\"share\" + 0.011*\"victoria\" + 0.010*\"business\" + 0.010*\"war\" + 0.010*\"news\" + 0.009*\"bank\" + 0.009*\"show\"\n",
      "Topic: 2 \n",
      "Words: 0.018*\"child\" + 0.016*\"sex\" + 0.016*\"family\" + 0.014*\"life\" + 0.013*\"tasmanian\" + 0.013*\"new\" + 0.011*\"missing\" + 0.011*\"year\" + 0.010*\"royal\" + 0.009*\"victim\"\n",
      "Topic: 3 \n",
      "Words: 0.031*\"australia\" + 0.025*\"election\" + 0.025*\"world\" + 0.023*\"south\" + 0.021*\"sydney\" + 0.012*\"record\" + 0.009*\"say\" + 0.009*\"cup\" + 0.009*\"program\" + 0.008*\"refugee\"\n",
      "Topic: 4 \n",
      "Words: 0.024*\"win\" + 0.022*\"queensland\" + 0.022*\"day\" + 0.019*\"adelaide\" + 0.018*\"north\" + 0.015*\"country\" + 0.013*\"australia\" + 0.013*\"final\" + 0.012*\"west\" + 0.011*\"test\"\n",
      "Topic: 5 \n",
      "Words: 0.017*\"national\" + 0.016*\"hour\" + 0.011*\"power\" + 0.010*\"violence\" + 0.010*\"talk\" + 0.010*\"park\" + 0.008*\"shark\" + 0.008*\"new\" + 0.008*\"water\" + 0.008*\"david\"\n",
      "Topic: 6 \n",
      "Words: 0.011*\"dog\" + 0.011*\"return\" + 0.011*\"support\" + 0.010*\"station\" + 0.010*\"abc\" + 0.009*\"home\" + 0.008*\"rugby\" + 0.008*\"centre\" + 0.008*\"take\" + 0.007*\"hold\"\n",
      "Topic: 7 \n",
      "Words: 0.014*\"turnbull\" + 0.013*\"ban\" + 0.011*\"call\" + 0.010*\"victorian\" + 0.010*\"law\" + 0.009*\"drum\" + 0.009*\"liberal\" + 0.009*\"marriage\" + 0.009*\"body\" + 0.009*\"vote\"\n",
      "Topic: 8 \n",
      "Words: 0.041*\"man\" + 0.035*\"police\" + 0.024*\"woman\" + 0.024*\"court\" + 0.017*\"attack\" + 0.015*\"murder\" + 0.015*\"charged\" + 0.014*\"crash\" + 0.014*\"car\" + 0.013*\"death\"\n",
      "Topic: 9 \n",
      "Words: 0.025*\"trump\" + 0.015*\"nsw\" + 0.013*\"council\" + 0.012*\"change\" + 0.012*\"health\" + 0.012*\"new\" + 0.012*\"plan\" + 0.012*\"rural\" + 0.012*\"school\" + 0.010*\"indigenous\"\n"
     ]
    }
   ],
   "source": [
    "for iloc, topic in lda_model_bow.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(iloc, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = models.LdaMulticore(corpus_tfidf,num_topics=10,id2word=dictionary,passes=2,workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.009*\"league\" + 0.009*\"afl\" + 0.008*\"world\" + 0.008*\"final\" + 0.008*\"win\" + 0.008*\"cup\" + 0.007*\"australia\" + 0.007*\"john\" + 0.007*\"malcolm\" + 0.006*\"rugby\"\n",
      "Topic: 1 \n",
      "Words: 0.016*\"market\" + 0.013*\"turnbull\" + 0.009*\"share\" + 0.009*\"podcast\" + 0.007*\"australian\" + 0.007*\"grandstand\" + 0.007*\"dollar\" + 0.006*\"october\" + 0.006*\"syria\" + 0.006*\"wednesday\"\n",
      "Topic: 2 \n",
      "Words: 0.014*\"donald\" + 0.006*\"peter\" + 0.005*\"islamic\" + 0.005*\"monday\" + 0.005*\"thursday\" + 0.005*\"kill\" + 0.005*\"tony\" + 0.005*\"australia\" + 0.005*\"australian\" + 0.005*\"island\"\n",
      "Topic: 3 \n",
      "Words: 0.010*\"christmas\" + 0.008*\"rio\" + 0.007*\"wall\" + 0.007*\"street\" + 0.006*\"truck\" + 0.006*\"november\" + 0.006*\"andrew\" + 0.004*\"white\" + 0.004*\"origin\" + 0.004*\"disability\"\n",
      "Topic: 4 \n",
      "Words: 0.026*\"trump\" + 0.013*\"queensland\" + 0.012*\"north\" + 0.008*\"west\" + 0.007*\"south\" + 0.006*\"ash\" + 0.006*\"korea\" + 0.006*\"australia\" + 0.006*\"weather\" + 0.006*\"coast\"\n",
      "Topic: 5 \n",
      "Words: 0.017*\"country\" + 0.016*\"hour\" + 0.014*\"rural\" + 0.011*\"news\" + 0.006*\"nsw\" + 0.006*\"health\" + 0.006*\"hobart\" + 0.006*\"mental\" + 0.005*\"cancer\" + 0.005*\"national\"\n",
      "Topic: 6 \n",
      "Words: 0.013*\"tasmanian\" + 0.007*\"abc\" + 0.007*\"michael\" + 0.007*\"david\" + 0.006*\"energy\" + 0.006*\"sport\" + 0.006*\"mother\" + 0.006*\"care\" + 0.005*\"stabbing\" + 0.005*\"george\"\n",
      "Topic: 7 \n",
      "Words: 0.008*\"government\" + 0.007*\"marriage\" + 0.006*\"hill\" + 0.006*\"federal\" + 0.006*\"sex\" + 0.005*\"rate\" + 0.004*\"june\" + 0.004*\"abuse\" + 0.004*\"dairy\" + 0.004*\"asylum\"\n",
      "Topic: 8 \n",
      "Words: 0.010*\"drum\" + 0.010*\"live\" + 0.009*\"election\" + 0.008*\"royal\" + 0.007*\"commission\" + 0.007*\"liberal\" + 0.007*\"labor\" + 0.006*\"government\" + 0.006*\"story\" + 0.005*\"say\"\n",
      "Topic: 9 \n",
      "Words: 0.020*\"man\" + 0.015*\"police\" + 0.011*\"charged\" + 0.011*\"murder\" + 0.010*\"woman\" + 0.009*\"court\" + 0.009*\"death\" + 0.008*\"crash\" + 0.008*\"interview\" + 0.008*\"car\"\n"
     ]
    }
   ],
   "source": [
    "for iloc, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(iloc, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
